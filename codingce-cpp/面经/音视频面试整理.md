# 音视频面试整理

## H264/H265有什么区别？

同样的画质和同样的码率，H.265比H2.64 占用的存储空间要少理论50%。如果存储空间一样大，那么意味着，在一样的码率下H.265会比H.264 画质要高一些理论值是30%~40%。

比起H.264，H.265提供了更多不同的工具来降低码率，以编码单位来说，最小的8x8到最大的64x64。信息量不多的区域(颜色变化不明显)划分的宏块较大，编码后的码字较少，而细节多的地方划分的宏块就相应的小和多一些，编码后的码字较多，这样就相当于对图像进行了有重点的编码，从而降低了整体的码率，编码效率就相应提高了。

H.265标准主要是围绕着现有的视频编码标准H.264，在保留了原有的某些技术外，增加了能够改善码流、编码质量、延时及算法复杂度之间的关系等相关的技术。H.265研究的主要内容包括，提高压缩效率、提高鲁棒性和错误恢复能力、减少实时的时延、减少信道获取时间和随机接入时延、降低复杂度。

## 视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？

选择UDP协议，UDP实时性好。TCP要保证丢失的package会被再次重发，确保对方能够收到。 而在视频播放中，如果有一秒钟的信号确实，导致画面出现了一点瑕疵，那么最合适的办法是把这点瑕疵用随便哪些信号补充上，这样虽然画面有一点点瑕疵但是不影响观看。如果用的TCP的话，这点缺失的信号会被一遍又一遍的发送过来直到接收端确认收到。这不是音视频播放所期待的。而UDP就很适合这种情况。UDP不会一遍遍发送丢失的package。

## 平时说的软解和硬解，具体是什么？

硬解就是硬件解码，指利用GPU来部分代替CPU进行解码，软解就是软件解码，指利用软件让CPU来进行解码。两者的具体区别如下所示：

**硬解码**：是将原来全部交由CPU来处理的视频数据的一部分交由GPU来做，而GPU的并行运算能力要远远高于CPU，这样可以大大的降低对CPU的负载，CPU的占用率较低了之后就可以同时运行一些其他的程序了，当然，对于较好的处理器来说，比如i5 2320，或者AMD 任何一款四核心处理器来说，硬解和软件的区别只是个人偏好问题了吧。　　

**软解码**：即通过软件让CPU来对视频进行解码处理；而硬解码：指不借助于CPU，而通过专用的子卡设备来独立完成视频解码任务。曾经的VCD/DVD解压卡、视频压缩卡等都隶属于硬解码这个范畴。而现如今，要完成高清解码已经不再需要额外的子卡，因为硬解码的模块已经被整合到显卡GPU的内部，所以目前的主流显卡（集显）都能够支持硬解码技术。

## 何为直播？何为点播？

直播：是一个三方交互(主播、服务器、观众)，这个交互式实时的！尽管会根据选择的协议不同而有一些延迟，但我们仍认为它直播是实时的！--->主播在本地发送音视频给服务器（推流），观众从服务器实时解码（拉流）收看收听主播发送给服务器的音视频（直播内容）。直播是不能快进的点播：首先一定要明确的一点，点播不存在推流这一过程，你本身你的流已经早就推给服务器了，或者这么说也不对，应该是你的音视频早就上传到了服务器，观众只需要在线收看即可，由于你的音视频上传到了服务器，观众则可以通过快进，快退，调整进度条等方式进行收看！ 

## 简述推流、拉流的工作流程？

推流：在直播中，一方向服务器发送请求，向服务器推送自己正在实时直播的数据，而这些内容在推送到服务器的这一过程中是以 “流” 的形式传递的，这就是“推流”，把音视频数据以流的方式推送（或上传）到服务器的过程就是“推流”！ 推流方的音视频往往会很大，在推流的过程中首先按照 aac音频-编码 和 h264视频-编码的标准把推过来的音视频压缩 ，然后合并成 MP4或者 FLV格式，然后根据直播的封装协议，最后传给服务器完成推流过程。 

拉流：与推流正好相反，拉流是用户从服务器获取推流方给服务器的音视频的过程，这就是“拉流”！拉流首先aac音频-解码 和 h.264视 频-解码的内部把推过来的音视频解压缩，然后合成 MP4或者 FLV 格式，再解封装，最后到我们的客户端与观众进行交互。

## 为什么巨大的原始视频可以编码成很小的视频呢?这其中的技术是什么呢?

- 空间冗余：图像相邻像素之间有较强的相关性
- 时间冗余：视频序列的相邻图像之间内容相似
- 编码冗余：不同像素值出现的概率不同
- 视觉冗余：人的视觉系统对某些细节不敏感
- 知识冗余：规律性的结构可由先验知识和背景知识得到

## 图像可以提取的特征有哪些？

颜色、纹理（粗糙度、方向度、对比度）、形状（曲率、离心率、主轴方向）、色彩等。

## AAC和PCM的区别？

AAC在数据开始时候加了一些参数：采样率、声道、采样大小

## H264存储的两个形态？

- a. Annex B :

> StartCode ：NALU单元，开头一般是0001或者001
> 防竞争字节：为了区分 0 0 0 1，它采用0 0 0 0x3 1作为区分
> 多用于网络流媒体中：rtmp、rtp格式

- b. AVCC :

> 表示NALU长度的前缀，不定长用4、2、1来存储这个NALU的长度
> 防竞争字节
> 多用于文件存储中mp4的格式

## FFMPEG：图片如何合成视频

编码流程：

1. av_register_all
2. 为AVFormatContext 分配内存
3. 打开文件
4. 创建输出码流AVSream
5. 找到编码器
6. 打开编码器
7. 写文件头，没有的就不写入
8. 循环编码视频像素数据->视频压缩数据
9. 将编码后的视频码流写入文件 ——>AVPacket转化为AVFormat函数
10. 关闭编码器
11. 写文件尾
12. 关闭资源文件

解码流程：

1. av_register_all
2. 创建AVFormatContext的对象上下文
3. 打开文件
4. avformat_find_stream_info
5. 找到解码器
6. 打开解码器
7. 创建AVCodecContext上下文
8. av_read_frame ：将avPacket数据转换为avFrame数据，glUniform1i() ——>这个可以设置对应纹理的第几层 glTexSubImage2D() 和glTexImage2D区别————>替换纹理的内容

## 常见的音视频格式有哪些？

1. MPEG（运动图像专家组）是Motion Picture Experts Group 的缩写。这类格式包括了MPEG-1,MPEG-2和MPEG-4在内的多种视频格式。
2. AVI，音频视频交错(Audio Video Interleaved)的英文缩写。AVI这个由微软公司发布的视频格式，在视频领域可以说是最悠久的格式之一。
3. MOV，使用过Mac机的朋友应该多少接触过QuickTime。QuickTime原本是Apple公司用于Mac计算机上的一种图像视频处理软件。
4. ASF(Advanced Streaming format高级流格式)。ASF 是MICROSOFT 为了和的Real player 竞争而发展出来的一种可以直接在网上观看视频节目的文件压缩格式。
5. WMV，一种独立于编码方式的在Internet上实时传播多媒体的技术标准，Microsoft公司希望用其取代QuickTime之类的技术标准以及WAV、AVI之类的文件扩展名。
6. NAVI，如果发现原来的播放软件突然打不开此类格式的AVI文件，那你就要考虑是不是碰到了n AVI。n AVI是New AVI 的缩写，是一个名为Shadow Realm 的地下组织发展起来的一种新视频格式。
7. 3GP是一种3G流媒体的视频编码格式，主要是为了配合3G网络的高传输速度而开发的，也是目前手机中最为常见的一种视频格式。
8. REAL VIDEO（RA、RAM）格式由一开始就是定位在视频流应用方面的，也可以说是视频流技术的始创者。
9. MKV，一种后缀为MKV的视频文件频频出现在网络上，它可在一个文件中集成多条不同类型的音轨和字幕轨，而且其视频编码的自由度也非常大，可以是常见的DivX、XviD、3IVX，甚至可以是RealVideo、QuickTime、WMV 这类流式视频。
10. FLV是FLASH VIDEO的简称，FLV流媒体格式是一种新的视频格式。由于它形成的文件极小、加载速度极快，使得网络观看视频文件成为可能，它的出现有效地解决了视频文件导入Flash后，使导出的SWF文件体积庞大，不能在网络上很好的使用等缺点。
11. F4V，作为一种更小更清晰，更利于在网络传播的格式，F4V已经逐渐取代了传统FLV，也已经被大多数主流播放器兼容播放，而不需要通过转换等复杂的方式。

## 视频分量YUV的意义及数字化格式？

4:2:0；4:1:1；4:2:2；4:4:4；多种

## sps和pps的区别？

SPS是序列参数集 0x67 PPS是图像参数集 0x68 在SPS序列参数集中可以解析出图像的宽，高和帧率等信息。而在h264文件中，最开始的两帧数据就是SPS和PPS，这个h264文件只存在一个SPS帧和一个PPS帧。

## 说一说ffmpeg的数据结构？

ffmpeg的数据结构可以分为以下几类： 

![](https://cdn.jsdelivr.net/gh/xzMhehe/StaticFile_CDN/static/img202212251952072.png)

- (1)解协议（http,rtsp,rtmp,mms） AVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。 URLProtocol存储输入音视频使用的封装格式。每种协议都对应一个URLProtocol结构。（注意：FFMPEG中文件也被当做一种协议“file”）
- (2)解封装（flv,avi,rmvb,mp4） AVFormatContext主要存储视音频封装格式中包含的信息 ffmpeg支持各种各样的音视频输入和输出文件格式（例如FLV, MKV, MP4, AVI），而 AVInputFormat和AVOutputFormat 结构体则保存了这些格式的信息和一些常规设置。
- (3)解码（h264,mpeg2,aac,mp3） AVStream是存储每一个视频/音频流信息的结构体。 AVCodecContext: 编解码器上下文结构体，存储该视频/音频流使用解码方式的相关数据。 AVCodec: 每种视频（音频）编解码器(例如H.264解码器)对应一 个该结构体。 三者的关系如下图:

![](https://cdn.jsdelivr.net/gh/xzMhehe/StaticFile_CDN/static/img202212251951812.png)

- (4)存数据 对于视频，每个结构一般是存一帧；音频可能有好几帧
  - 解码前数据：AVPacket
  - 解码后数据：AVFrame

## 说一说AVFormatContext 和 AVInputFormat之间的关系？

- AVInputFormat被封装在AVFormatContext里
- AVFormatContext 作为API被外界调用
- AVInputFormat 主要是FFmpeg内部调用
- AVFormatContext里保存了视频文件封装格式相关信息，它是负责储存数据的结构体。而AVInputFormat代表了各个封装格式，属于方法，这是一种面向对象的封装。

## FFmpeg 命令行 图片和视频缩放处理

指定输出视频的宽高

```bash
ffmpeg -i input.mp4 -vf scale=720:480 -y output.mp4
```

指定输出视频的宽，高度根据原始图片宽高比例进行缩放

```bash
ffmpeg -i input.mp4 -vf scale=720:-1  -y output.mp4
ffmpeg -i input.mp4 -vf scale=-1:480 -y output.mp4
```

如果是要求3的倍数，指令如下：
```bash
ffmpeg -i input.mp4 -vf scale=720:-3  -y output.mp4
```

通过iw和ih指定输出视频的宽度和高度（iw =输入宽度，ih =输入高度）
宽度拉伸一倍

```bash
ffmpeg -i input.mp4 -vf scale=iw*2:ih  -y output.mp4
```

高度压缩一半
```bash
ffmpeg -i input.mp4 -vf scale=iw:ih/2  -y output.mp4
```

限制拉伸的最大宽度480和最大高度360
```bash
ffmpeg -i input.mp4 -vf  "scale='min(480, iw)': 'min(360, ih)'"  -y output.mp4
```

填充黑色边框
```bash
ffmpeg -i input.mp4 -vf  "scale=720:480:force_original_aspect_ratio=decrease,pad=720:480:(ow-iw)/2:(oh-ih)/2" output1.mp4
```

指定缩放算法
```bash
ffmpeg -i input.mp4 -vf scale=iw:ih/2 -sws_flags bilinear  -y output.mp4
```

图片和视频处理方式是一样的，只需要替换输入输出文件即可

```bash
ffmpeg -i input.png -vf scale=720:480 -y output.png
```

## FFmpeg音频重采样

**音频重采样**就是改变音频的采样率、采样格式、声道数等参数，使之按照我们期望的参数输出。比如我们将采样率 48kHz、采样格式 f32le、声道数 1 的音频 A 转换成采样率 44.1kHz、采样格式 s16le、声道数 2 的音频 B。

那么为什么需要对音频重采样？列举一个经典用途，有些音频编码器对输入的原始PCM数据是有特定参数要求的，比如要求必须是44100_s16le_2。但是你提供的PCM参数可能是48000_f32le_1。这个时候就需要先将48000_f32le_1转换成44100_s16le_2，然后再使用音频编码器对转换后的PCM进行编码。

**使用 FFmpeg 命令行实现音频重采样**：

将采样率 48000 采样格式 s32le 声道数 1 的 PCM 音频数据重采样成采样率 44100 采样格式 s16le 声道数 2 的 PCM 音频数据:

```bash
$ ffmpeg -ar 48000 -ac 1 -f f32le -i ar48000ac1f32le.pcm -ar 44100 -ac 2 -f s16le ar44100ac2s16le.pcm
```

使用 libavresample 音频重采样的核心步骤:

<img src="https://cdn.jsdelivr.net/gh/xzMhehe/StaticFile_CDN/static/img202212252006841.png" style="zoom: 50%;" />

## DTS与PTS

- PTS就是Presentation Time Stamp也就说这个帧什么时候会放在显示器上;
- DTS就是Decode Time Stamp，就是说这个帧什么时候被放在编码器去解。 在没有B帧的情况下，DTS和PTS的输出顺序是一样的。

## 影响视频清晰度的指标有哪些

帧率 码率 分辨率 量化参数（压缩比）

## 为什么要有YUV这种数据出来？（YUV相比RGB来说的优点）

RGB是指光学三原色红、绿和蓝，通过这3种的数值（0-255）改变可以组成其他颜色，全0时为黑色，全255时为白色。RGB是一种依赖于设备的颜色空间：不同设备对特定RGB值的检测和重现都不一样，因为颜色物质（荧光剂或者染料）和它们对红、绿和蓝的单独响应水平随着制造商的不同而不同，甚至是同样的设备不同的时间也不同。

YUV，是一种颜色编码方法。常使用在各个视频处理组件中。三个字母分别表示亮度信号Y和两个色差信号R－Y（即U）、B－Y（即V），作用是描述影像色彩及饱和度，用于指定像素的颜色。Y'UV的发明是由于彩色电视与黑白电视的过渡时期。黑白视频只有Y视频，也就是灰阶值。与我们熟知的RGB类似，YUV也是一种颜色编码方法，主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV和RGB是可以相互转换的，基本上所有图像算法都是基于YUV的，所有显示面板都是接收RGB数据。

